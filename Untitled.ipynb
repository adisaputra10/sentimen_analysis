{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle \n",
    "from sklearn import svm \n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm \n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dictionary ... \n",
      "Complate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Preparing data ...\n",
      "Complate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pipelining process ...\n",
      "<function get_fitur at 0x7f45cacc8400>\n",
      "2122\n",
      "Complate \n",
      "\n",
      "\n",
      "classfication ...\n",
      "Complate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ambil kamus stopword dalam class preprocessing\n",
    "print (\"loading dictionary ... \")\n",
    "stop_words = [(x.strip(), 'utf-8') for x in open('kamus/stopword.txt','r').read().split('\\n')]\n",
    "noise = [(x.strip(), 'utf-8') for x in open('kamus/noise.txt','r').read().split('\\n')]\n",
    "stop_words.extend(noise)\n",
    "print (\"Complate\")\n",
    "print (\"\\n\")\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "#persiapan data testing dan training\n",
    "print (\"Preparing data ...\")\n",
    "train_df_raw = pd.read_csv('dataset_final/training90.csv',sep=';',names=['tweets','label'],header=None)\n",
    "test_df_raw = pd.read_csv('dataset_final/testing1.csv',sep=';',names=['tweets','label'],header=None)\n",
    "train_df_raw = train_df_raw[train_df_raw['tweets'].notnull()]\n",
    "test_df_raw = test_df_raw[test_df_raw['tweets'].notnull()]\n",
    "\n",
    "print (\"Complate\")\n",
    "print (\"\\n\")\n",
    "print (\"\\n\")\n",
    "\n",
    "#ambil data training\n",
    "X_train=train_df_raw['tweets'].tolist()\n",
    "\n",
    "#sample preprocessing \n",
    "# for tweet in X_train: \n",
    "# \ttweets=tweet\n",
    "# \tpre=preprocessing.preprocess(tweets)\n",
    "# \tfitur=preprocessing.get_fitur_all(pre)\n",
    "# \tprint fitur\n",
    "\n",
    "#ambil data testing\n",
    "X_test=test_df_raw['tweets'].tolist()\n",
    "# print X_train\n",
    "# print X_test\n",
    "\n",
    "#ambil label \n",
    "y_train=[x if x==1 else 0 for x in train_df_raw['label'].tolist()]\n",
    "\n",
    "#tanpa cross validation , manual label (unseen data)\n",
    "#y_test=[x if x=='positif' else 'negatif' for x in test_df_raw['label'].tolist()]\n",
    "print (\"Pipelining process ...\")\n",
    "\n",
    "#proses pembobotan tf-idf \n",
    "vectorizer = TfidfVectorizer(max_df=1.0, max_features=10000,\n",
    "                             min_df=0, preprocessor=preprocessing.preprocess,\n",
    "                             tokenizer=preprocessing.get_fitur\n",
    "                            )\n",
    "# vectorizer = TfidfVectorizer(max_df=1.0, max_features=10000,\n",
    "#                              min_df=0, preprocessor=preprocessing.preprocess,\n",
    "#                              stop_words=stop_words,vocabulary=preprocessing.get_fitur\n",
    "#                             )\n",
    "#fitur setalah dilakukan pembobotan \n",
    "X_train=vectorizer.fit_transform(X_train).toarray()\n",
    "X_test=vectorizer.transform(X_test).toarray()\n",
    "\n",
    "#fitur \n",
    "feature_names=vectorizer.get_feature_names()\n",
    "# idf=vectorizer.idf_\n",
    "#tampilkan fitur \n",
    "print (preprocessing.get_fitur)\n",
    "#jumlah fitur \n",
    "print (len(feature_names))\n",
    "#menampilkan fitur yang sudah di tf-idf \n",
    "# print dict(zip(vectorizer.get_feature_names(), idf))\n",
    "# print len(vectorizer.get_feature_names(),idf)\n",
    "\n",
    "\n",
    "#Hitung jumlah fitur \n",
    "# print len(X_train)\n",
    "# print len(X_test)\n",
    "\n",
    "print (\"Complate \" )\n",
    "print (\"\\n\")\n",
    "print (\"classfication ...\")\n",
    "\n",
    "#klasifikasi support vector machine \n",
    " \n",
    "\n",
    "\n",
    "clf=svm.SVC(kernel='linear',gamma=1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#simpan data training \n",
    "\n",
    "#filesave='save_train/svmlinear9010.sav'\n",
    "#pickle.dump(clf,open(filesave,'wb'))\n",
    "#clf = pickle.load(open(filesave, 'rb'))\n",
    "print (\"Complate\")\n",
    "print (\"\\n\")\n",
    "#train model \n",
    "skf=StratifiedKFold(n_splits=5,random_state=0)\n",
    "scores=cross_val_score(clf,X_train,y_train,cv=skf)\n",
    "precision_score=cross_val_score(clf,X_train,y_train,cv=skf,scoring='precision')\n",
    "recall_score=cross_val_score(clf, X_train,y_train, cv=skf, scoring ='recall')\n",
    "\n",
    "#scoring                                                                                                                                                                                                                                             b                                                                                                                                                                                                                  \n",
    "print (\"Result ...\")\n",
    "print (\"Recall :%0.2f\"%recall_score.mean())\n",
    "print (\"Precision :%0.2f\"%precision_score.mean())\n",
    "print (\"Accuracy :%0.2f\"%scores.mean())\n",
    "\n",
    "#prosentase grafik\n",
    "weighted_prediction=clf.predict(X_test)\n",
    "#print len(weighted_prediction)\n",
    "\n",
    "\"\"\"\n",
    "c=Counter(weighted_prediction)\n",
    "plt.bar(c.keys(),c.values())\n",
    "\"\"\"\n",
    "\n",
    "#ambil nilai prediksi dari variabel weighted_prediction \n",
    "labels, values = zip(*Counter(weighted_prediction).items())\n",
    "indexes=np.arange(len(labels))\n",
    "width=0.9\n",
    "#print collections.Counter(weighted_prediction)\t \n",
    "labels, values = zip(*Counter(weighted_prediction).items())\n",
    "SentimenPositif=values[1]\n",
    "SentimenNegatif=values[0]\n",
    "print(SentimenPositif)\n",
    "print(SentimenNegatif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
